{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4b7163-75aa-4fb4-8424-f91ff5988582",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd0e8a-425c-4f92-9a75-5280074bf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "from responsive_voice.voices import UKEnglishMale\n",
    "\n",
    "from inference import FaceDetection, MaskDetection\n",
    "from pyvino_utils import InputFeeder\n",
    "from IPython import display\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d64b1-616b-4b38-a9cd-396ead86facc",
   "metadata": {},
   "source": [
    "### Set the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314f518-869f-47b1-94cb-2b849e91141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setargs():\n",
    "    face_model = \"models/face-detection-adas-0001\"\n",
    "    mask_model = \"models/face_mask\"\n",
    "    input = \"cam\"\n",
    "    device = \"GPU\"\n",
    "    face_prob_threshold = 0.8\n",
    "    mask_prob_threshold = 0.3\n",
    "    enable_speech = False\n",
    "    tts = \"Please wear your MASK!!\"; # Text-to-Speech, used for notification.\n",
    "    ffmpeg = False; # Flush video to FFMPEG\n",
    "    show_bbox = True; # Show bounding box and stats on screen [debugging].\n",
    "    debug = True; # Show output on screen [debugging].\n",
    "    width = 640\n",
    "    height = 360\n",
    "    \n",
    "args = setargs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4b95b-800e-477f-8faa-02089f489bae",
   "metadata": {},
   "source": [
    "### Run the inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908602ba-364a-452d-a99c-269f39a0e652",
   "metadata": {},
   "source": [
    "#### Initialize the video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cb78e-4431-4177-819f-1b9ce8dfd784",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feed = InputFeeder(input_feed=args.input)\n",
    "input_feed.resize_cam_input(args.height, args.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0314dc-969c-4919-9f78-5ae791a7d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for frame in input_feed.next_frame():\n",
    "        # encode numpy array to jpg\n",
    "    _, encoded_img = cv2.imencode(\n",
    "        ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "    )\n",
    "    i = display.Image(data=encoded_img)\n",
    "    # Display the image in this notebook\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c801d-cf7e-4277-ad95-6b891a1d1810",
   "metadata": {},
   "source": [
    "#### Enable speech output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3add3f-ea83-40a9-b6a2-e92f659f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.enable_speech:\n",
    "    # TODO: Add args for selecting language, accent and male/female voice\n",
    "    engine = UKEnglishMale()\n",
    "    speak = engine.get_mp3(args.tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e8c1c-05db-47ac-a1fe-0af8faeca69c",
   "metadata": {},
   "source": [
    "#### Load the face detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834690d-7621-4d77-b1be-b1d5c3df7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = FaceDetection(\n",
    "    model_name=args.face_model,\n",
    "    device=args.device,\n",
    "    threshold=args.face_prob_threshold,\n",
    "    input_feed=input_feed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056ea0c-b7ce-499c-bfcb-57acf17cafae",
   "metadata": {},
   "source": [
    "#### Load the mask detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf406370-b419-4879-b3d6-105fca1e9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection = MaskDetection(\n",
    "    model_name=args.mask_model,\n",
    "    device=args.device,\n",
    "    threshold=args.mask_prob_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1642621-1ad3-49be-a56b-5f1442eb14c8",
   "metadata": {},
   "source": [
    "#### Now, the inferencing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec49a7-f221-417b-a382-32eb2b116977",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "face_detect_infer_time = 0\n",
    "mask_detect_infer_time = 0\n",
    "mask_detected_prob = -1\n",
    "try:\n",
    "    # TODO: Convert to contextmanager\n",
    "    face_detect_infer_time_samples = []\n",
    "    mask_detect_infer_time_samples = []\n",
    "    for frame in input_feed.next_frame():\n",
    "        count += 1\n",
    "\n",
    "        start_time = time.time()\n",
    "        fd_results = face_detection.predict(\n",
    "            frame, show_bbox=args.show_bbox, mask_detected=mask_detected_prob\n",
    "        )\n",
    "        face_detect_infer_time_samples.append(time.time() - start_time)\n",
    "        face_bboxes = fd_results[\"process_output\"][\"bbox_coord\"]\n",
    "        if face_bboxes:\n",
    "            for face_bbox in face_bboxes:\n",
    "                # Useful resource:\n",
    "                # https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/\n",
    "\n",
    "                # Face bounding box coordinates cropped from the face detection\n",
    "                # inference are face_bboxes i.e `xmin, ymin, xmax, ymax`\n",
    "                # Therefore the face can be cropped by:\n",
    "                # frame[face_bbox[1]:face_bbox[3], face_bbox[0]:face_bbox[2]]\n",
    "\n",
    "                # extract the face ROI\n",
    "                (x, y, w, h) = face_bbox\n",
    "                face = frame[y:h, x:w]\n",
    "                (face_height, face_width) = face.shape[:2]\n",
    "                # Crop and show face\n",
    "                # input_feed.show(frame[y:h, x:w], \"face\")\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if face_height < 20 or face_width < 20:\n",
    "                    continue\n",
    "\n",
    "                start_time = time.time()\n",
    "                md_results = mask_detection.predict(\n",
    "                    face, show_bbox=args.show_bbox, frame=frame\n",
    "                )\n",
    "                mask_detect_infer_time_samples.append(time.time() - start_time)\n",
    "                mask_detected_prob = md_results[\"process_output\"][\n",
    "                    \"flattened_predictions\"\n",
    "                ]\n",
    "                if (\n",
    "                    int(count) % 200 == 1\n",
    "                    and args.enable_speech\n",
    "                    and float(mask_detected_prob) < args.mask_prob_threshold\n",
    "                ):\n",
    "                    engine.play_mp3(speak)\n",
    "                    \n",
    "        # Calculate the avg inferecing time every 200 samples\n",
    "        if int(count) % 200 == 1:\n",
    "            if len(face_detect_infer_time_samples):\n",
    "                face_detect_infer_time = np.mean(face_detect_infer_time_samples) * 1000\n",
    "                face_detect_infer_time_samples = []\n",
    "            else:\n",
    "                face_detect_infer_time = 0\n",
    "            if len(mask_detect_infer_time_samples):                        \n",
    "                mask_detect_infer_time = np.mean(mask_detect_infer_time_samples) * 1000\n",
    "                mask_detect_infer_time_samples = []\n",
    "            else:\n",
    "                mask_detect_infer_time = 0\n",
    "                    \n",
    "        if args.debug:\n",
    "            text = f\"Face Detection Inference time: {face_detect_infer_time:.3f} ms\"\n",
    "            input_feed.add_text(text, frame, (15, input_feed.source_height - 80))\n",
    "            text = (\n",
    "                f\"Face Mask Detection Inference time: {mask_detect_infer_time:.3f} ms\"\n",
    "            )\n",
    "            input_feed.add_text(text, frame, (15, input_feed.source_height - 60))\n",
    "            \n",
    "        # input_feed.show(input_feed.resize(frame))\n",
    "        frame = input_feed.resize(frame)\n",
    "        _, encoded_img = cv2.imencode(\n",
    "            ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "        )\n",
    "        i = display.Image(data=encoded_img)\n",
    "        # Display the image in this notebook\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(i)\n",
    "\n",
    "finally:\n",
    "    input_feed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02fe5e-3814-45ed-b928-759c3f99bb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
